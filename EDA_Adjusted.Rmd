---
title: "EDA"
author: "Amjad Altuwayjiri"
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    number_sections: no
    toc: yes
    toc_depth: 3
    toc_float: yes

---
# Data loading and structure
```{r, include=FALSE}

library(ggplot2)
library(dplyr)
library(tidyr)
library(RColorBrewer)
library(lubridate)
library(ezids)
# library(sf)

```

```{r setup, include=FALSE}

getwd()
bike <- data.frame(read.csv("bo.csv"))

```

### How the data look like (Summary of the dataset)
Totally there are `r length(bike)` columns and `r nrow(bike)` rows, and `r length(bike)*nrow(bike)` observations in this dataframe.
Below are all the column names of the dataframe, of which ride_id refers to......etc (Presentation only or also in the R-Code?). 
```{r}
colnames(bike)
```
Below are the overview structure of the whole dataframe. 
```{r}

str(bike)
head(bike, n=4)

```



# Data preparing
### Dealing with NA values for the whole dataframe
```{r, results='markup'}
print(paste(sum(is.na(bike)), "number of NA in the data"))
sapply(bike, function(y) sum(length(which(is.na(y)))))
```

Most of Na in the columns `start_station_id and` and `end_station_id`. However, since the name of the station is present I don't think we should delete the whole row! and for further investigation I subset the NA values to track any pattern. 
```{r, results='markup'}

bike_1 <- subset(bike, !is.na(start_station_id)) 
bike_final <- subset(bike_1, !is.na(end_station_id))
```
Then, we will check if there still have NA-Values in the dataframe. 
```{r, results='markup'}

print(paste(sum(is.na(bike_final)), "number of NA in the data"))
sapply(bike_final, function(y) sum(length(which(is.na(y)))))

```
And below is the data structure after cleaning all the null values：
```{r}

print("Columns' names are：")
colnames(bike_final)

print('')
print('')

print("Structure of the cleaned data is：")
str(bike_final)

```


```{r,results='markup'}
#subdset the nas in end_station_id
#navalues <- bike[bike$end_station_id[which(bike$end_station_id == NA),]]
#head(navalues)
```

We can see a repetition of `end_station_name` and `start_station_name` so lets check if the station names are present in the main data `bike`?

```{r}
#check if the nas has equivilant in the main data
#station_name <- bike$start_station_name %in% navalues$start_station_name
station1 <- subset(bike, end_station_name == "Scioto Audubon Center", select=c(end_station_name, end_station_id))

#same_name<-sum(station_name[station_name == TRUE])
```
I created a boolian value to check if the name of station in the subset is present in the main dataset.

### Change columns type  
```{r, results='markup'}
# change rideable_type/ member_casual to factor 
#there three numbers under the factor rideable_type needs to look into!
bike$rideable_type <- factor(bike$rideable_type)
bike$member_casual <- factor(bike$member_casual)

# split started_at/ ended_at to date column and time column
# change start_date/end_date to date type
bike$start_time <- format(as.POSIXct(bike$started_at), format = "%H:%M:%S") 
bike$end_time <- format(as.POSIXct(bike$ended_at), format = "%H:%M:%S") 
bike$start_date <- as.Date(bike$started_at)
bike$end_date <- as.Date(bike$ended_at)

# Check the structure again
str(bike)
```



# Data analysis & visualazation

### How many users of each membership type we have?
```{r, results='markup'}
bike %>% count(member_casual)
ggplot(bike, aes(member_casual, fill = member_casual))+
  geom_bar()+
  scale_fill_brewer(palette = "BuPu")+
  guides(fill="none")+
  labs(title = "User membersip types", x= "types of memebership")+
  theme_classic()
```

There are more causal (24-hour pass or 3-day pass user) than annual members users by around 1000 difference in August 2022. There are two types of the Causal member which are Single trip cost 2.25 per 30m and 8 for unlimited 30m ride in a day, annual membership on the other hand cost 85$ a year.

### What is the most frequent bike type used?
```{r, results='markup'}
ggplot(bike, aes(rideable_type, fill = rideable_type))+
  geom_bar()+
  scale_fill_brewer(palette = "BuPu")+
  guides(fill="none")+
  labs(title = "Types of used biks", x = "")+
  theme_classic()
  
```

There are few users of docked bike type comparing to the others. Docked bike is a bicycles that can be borrowed or rented from an automated station or "docking stations". It is interesting why would people prefer other types above this type!

### casual members prefer watch type? VS members
```{r, results='markup'}
# grouping types of users and counting their used bike type without counting docked_bike because it is only 3 users
members_preferance <- bike %>% group_by(member_casual, rideable_type)%>%
  filter(rideable_type != "docked_bike")%>%
  summarise(used = n())
print(members_preferance)
ggplot(members_preferance, aes(x= member_casual,y = used , fill = rideable_type))+
  geom_bar(position='dodge', stat='identity')+
  scale_fill_brewer(palette = "BuPu")+
  labs(title = "Most used bike type to user", x= "type of user", y="")+
  theme_classic()
```

While there is no huge difference between annual members in choosing classic or electric bikes, Casual members choose to use electric bikes over the classic by around 680 user. 

### contingency table between customer type and bike type
```{r, results='markup'}
#the probability of each user to pick this type of bike
round(table(bike$member_casual, bike$rideable_type), 2)
```

While there is almost even number of the annual member to choose electric or classic bike,casual users are more likely to choose electric bike. 

### Which day of the week the serves is used more?
```{r, results='markup'}
#extrat only the day and convert it to day of the week
bike$days <- format(bike$start_date, format = "%a")
#convert it to a factor and organize the days order
bike$days <- factor(bike$days, levels = c("Sat", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri" ))


ggplot(bike, aes(days, fill = days))+
  geom_bar()+
  scale_fill_brewer(palette = "BuPu")+
  guides(fill="none")+
  labs(title = "Number of users in the days of the week", x="Days of the week")+
  theme_classic()
```

Saturdays and Wednesdays have the most number of users but overall there is no big difference between the days of the week in the count of users. 

### When is the highest-lowest time of use of the day?
```{r, results='markup'}
#get only the hour from the time
bike$hour <- NA 
bike$hour <- hour(bike$started_at)
sum_hour <- bike %>%
            group_by(hour) %>%
            summarise(sum_hour = length(hour)) 

ggplot(sum_hour, aes(hour, sum_hour ))+
  geom_line(color = "#8C6BB1", size = 1) +
  geom_point(color = "#8C96C6", size = 2) +
  scale_x_continuous(breaks=seq(0,23,1))+
  labs(title="Use by hour", y = "")+
  theme_classic()

```



### What is the hourly use of each day?
```{r, results='markup'}

sum_hour <- bike %>%
            group_by(days, hour)%>% summarise(count = n())

ggplot(data = sum_hour, aes(x = hour, y = count,  color = days))+
  geom_point() + geom_line(aes(group = 1))+
  facet_grid(rows = vars(days))+         
  scale_color_manual(values=c("#BFD3E6", "#9EBCDA" ,"#8C96C6" ,"#8C6BB1", "#88419D", "#810F7C", "#4D004B"))+
  labs(title= "Use by day and hour")+
  scale_y_continuous(breaks=seq(0,130,50))+ 
  scale_x_continuous(breaks=seq(0,23,1))+
    theme(
    plot.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank()
  )  

```


### Where are the most used stations

```{r, results='markup'}
#library(sf)
#ggplot()+ geom_sf(bike, aes(start_lat, start_lng))
#end_station$sum_att <- end_station%>% group_by(end_lat, end_lng) %>% summarise(count = length(end_lng))
#
#
#install.packages("mapview")
library(mapview)
#subset without the na 
end_station <- subset(bike, (!is.na(bike[,11])) & (!is.na(bike[,12])))
#if there is dincity change the color
#when chotching the point give me the name of the station 
#have the car for ohaio
mapview(bike, xcol = "start_lat", ycol = "start_lng", crs = 3735, grid = FALSE, lable = "Start Station")

mapview(end_station, xcol = "end_lat", ycol = "end_lng", crs = 3735, grid = FALSE)
#
#
#look <- st_as_sf(bike, coords = c("start_lat", "start_lng"),  crs = 4326)
#mapview(look, map.types = "Stamen.Toner") 
#library(ggmap)
#map_sf <- get_map('Ohio', zoom = 12, maptype = 'satellite')
#ggmap(map_sf) +
#  stat_density2d(data = bike, aes(x = start_lng, y = start_lat, fill = ..density..), geom = 'tile', contour = F, alpha = .5) +  
#  scale_fill_viridis()
```

```{r}

library(sp)
library(sf)
#> Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3
library(mapview)

test.coords<-as.data.frame(cbind(c(runif(15,-180,-130),runif(5,160,180)),runif(20,40,60)))
test.sp <- SpatialPointsDataFrame(coords = cbind(test.coords$V1,test.coords$V2), data = test.coords,
                                  proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))

test_sf = st_as_sf(test.sp)

shift = function(x) {
    geom = st_geometry(x)
    st_geometry(x) = st_sfc(
        lapply(seq_along(geom), function(i) {
            geom[[i]][1] = ifelse(geom[[i]][1] < 0, geom[[i]][1] + 360, geom[[i]][1])
            return(geom[[i]])
        })
        , crs = st_crs(geom)
    )
    return(x)
}

mapview(shift(test_sf)) +
    mapview(test_sf, col.regions = "orange")
```
**Is there a correlation between time and day of usage?**
```{r}
#glm(hour ~ days, data = bike, family = )

```
**What is the average trip distance?**
```{r}

```
**probopality of user membership and distance time**
```{r}
#y1 = contenuse, y2 = binary
#t.test(dis_time ~ member_casual, var.equal = FALSE)

```



# Summary of Statistics & Testing：
Before we are focusing on the qualitative and categorical variables (like locations and members) to analysis how we can improve our profits. For now, let's focus on analyzing the numeric variables (like price and time difference)：
### Summary Statistics 
First of all, let's look at the summary statistic of these two variables
```{r, results='markup'}
# New

library(readr)

getwd()
bike_2 <- data.frame(read.csv('Bike_time.csv'))
# bike_2$Total.Time = as.numeric(bike_2$Total.Time)
str(bike_2)

```

```{r, results=TRUE}

bike_time <- subset(bike_2, !is.na(Total.Time..int.), select=Total.Time..int.)

summary(bike_time)
#summary(bike_2$Total.Time..int., na.rm=TRUE)

```
According to the timing information statistics we can see that, overall, nearly 25% of customers use less than 7-minutes, nearly 50% of customers use less than 13-minutes, and nearly 75% of all customers use less than 25 minutes; 
According to this information, we can see we might have few percentage of customers who use more than 30 minutes；The timing price differentiate for different types of customers and different type of bike, so we may divide our data into different groups based on members_casual and bike_type to do the analysis in detailed；
Moreover, we have an max value 1499, we want to see which group it is located at and see if that's an occasional case for that group.

(Groups：
First, pure member,and, pure casual；pure classic, pure electric.
Then, in more detailed groups：classic & member,and, classic & casual；electric & member，and，electric & casual）

### Membership vs Casual Customer:
```{r, results=TRUE}

membership <- subset(bike_2, !is.na(Total.Time..int.)&member_casual=='member', select=c(member_casual, Total.Time..int.))

casual <- subset(bike_2, !is.na(Total.Time..int.)&member_casual=='casual', select=c(member_casual, Total.Time..int.))

print('Member_customers structure')
str(membership)
print('')
print('')
print('Casual_customers structure')
str(casual)
```
```{r, results=TRUE}

print('Summary Statistic for Members')
summary(membership$Total.Time..int.)
sd(membership$Total.Time..int.)
print('')
print('')
print('Summary Statistic for Casual')
summary(casual$Total.Time..int.)
sd(casual$Total.Time..int.)

```

```{r, results=TRUE}

library(ggplot2)

ggplot(data=membership, mapping = aes(Total.Time..int.))+geom_histogram(col='red', fill='blue')+labs(title='Histogram for membership customer timing')
qqnorm(membership$Total.Time..int., main='qqplot for membership timing', col='red')
qqline(membership$Total.Time..int., col='blue')

ggplot(data=casual, mapping = aes(Total.Time..int.))+geom_histogram(col='red', fill='blue')+labs(title='Histogram for casual customer timing')
qqnorm(casual$Total.Time..int., main='qqplot for casual timing', col='red')
qqline(casual$Total.Time..int., col='blue')
```
From the histogram of two types of customers we can see that, both of the distribuiton are extremely right-skewed, but we can see that there's not so much data distributed in the extreme right tail (we can also see that in the QQ-Plot because most of data are lying on the qqline, which are normally distributed), so we may say the extreme max values are happend occasionally and now we will now exclude the outliers and build the distribution again to see the results again. 

```{r, results=TRUE}

library(ggplot2)

membership_clean <- ezids::outlierKD2 (membership, Total.Time..int., qqplt = TRUE)
sd(membership_clean$Total.Time..int.)

casual_clean <- ezids::outlierKD2 (casual, Total.Time..int., qqplt = TRUE)
sd(casual_clean$Total.Time..int.)

```
After we clean out the outliers, we see the distribution (are still not totally normal distributed, so we decide to use t-distribution? use z-test seems to be more accurate) to check the probability that's more than the price boundaries (30 mins for casual & 45 mins for member)
```{r, results=TRUE}

library(ggplot2)

print('The proportion of casual customers use more than 30mins are about：')
pnorm(30, mean(casual_clean$Total.Time..int.), sd(casual_clean$Total.Time..int.),lower.tail=FALSE)
print('')
print('')
print('The proportion of member customers use more than 45mins are about：')
pnorm(45, mean(membership_clean$Total.Time..int.), sd(membership_clean$Total.Time..int.),lower.tail=FALSE)

```
According to the probability information, it seems like the casual customers are lessly care about the boundary price and will use much more time on members, if that's true, it means the company can increase their profit margin by slightly increasing the boundary price for casual members becasue casual customers may be more acceptable for the price incremental? 
To further test this assumption about casual customer always use more time than member, we need to test if the average time for using shared bike are the same with T-Test.
Firstly, we can use the averages in August to construct and compare two Confidence Interval, say the default confident level is 95%
(Because for now the August data is our sample so we need to use the T-Test to construct CI)
```{r, results=TRUE}

ttest1 <- t.test(membership_clean$Total.Time..int.)
ttest1$conf.int

ttest2 <- t.test(casual_clean$Total.Time..int.)
ttest2$conf.int

```
According to the CI Information above, se can see there's no overlap between these two CIs. 
Secondly, we can do a hypothesis testing between these 2 avearges. We set our null hypothesis H0: Two averages of total time are same, and just in case, we set our significance level as 5%.
(for a wider range, let's say august data is just a sample, but the data for the whole year is the population) 
```{r, results=TRUE}

ttest2sample_total_time <- t.test(casual_clean$Total.Time..int., membership_clean$Total.Time..int., mu=0, alternative = 'greater')
# Have to prove casual_clean at first argument because our alternative hypothesis is 'avg(casual)-avg(membership)>0' instead of 'avg(membership)-avg(casual)>0；if we put membership_clean at the first argument, then the result will have a extremely large p-value
ttest2sample_total_time

```
According to the p-value for differences in total time between two types of customers is 2.2e-16, because our p-value is extremely small, so the data presented is enough evidence to reject null hypothesis H0, and we can’t say the average riding time of membership is less than or the same as average riding time of casual customers.

### Classic Bike vs Electric Bike:
Except subsetting the time based on membership type, we can also subset the time based on bike type
```{r, results=TRUE}

classic_bike <- subset(bike_2, !is.na(Total.Time..int.)&rideable_type=='classic_bike', select=c(rideable_type, Total.Time..int.))

electric_bike <- subset(bike_2, !is.na(Total.Time..int.)&rideable_type=='electric_bike', select=c(rideable_type, Total.Time..int.))

print('classic_bike structure：')
str(classic_bike)
print('')
print('')
print('electric_bike structure：')
str(electric_bike)
```
```{r, results=TRUE}

print('Summary Statistic for classic bike')
summary(classic_bike$Total.Time..int.)
sd(classic_bike$Total.Time..int.)
print('')
print('')
print('Summary Statistic for electric bike')
summary(electric_bike$Total.Time..int.)
sd(electric_bike$Total.Time..int.)

```
Recall the time boundary is 30mins for extra price.
We can see that there's about less than 25% of people using more than 28mins for classic bike and more than 22 mins for electric bike, but these information was not enough, especially we haven't seen the probability for using more than the time boundary (30 minutes), so we need to check the distribution of these two data for more detailed information.
However, same as above, we saw the St.Deviation and variation is huge, this is another reason we want to look at the distribution for both classic bike and electric bike data to check if there's huge amount of outliers：
```{r, results=TRUE}

library(ggplot2)

ggplot(data=classic_bike, mapping = aes(Total.Time..int.))+geom_histogram(col='red', fill='blue')+labs(title='Histogram for classic bike using time')
qqnorm(classic_bike$Total.Time..int., main='qqplot for time of using classic bike', col='red')
qqline(classic_bike$Total.Time..int., col='blue')

ggplot(data=electric_bike, mapping = aes(Total.Time..int.))+geom_histogram(col='red', fill='blue')+labs(title='Histogram for electric bike using time')
qqnorm(electric_bike$Total.Time..int., main='qqplot for time of using electric bike', col='red')
qqline(electric_bike$Total.Time..int., col='blue')

```
We can see that both the data are highly right-skwed, which meanns there are some few extremely large outliers in the data, so we'll exclude those outliers first
```{r, results=TRUE}

library(ggplot2)

classic_bike_clean <- ezids::outlierKD2 (classic_bike, Total.Time..int., qqplt = TRUE)
sd(classic_bike_clean$Total.Time..int.)

electric_bike_clean <- ezids::outlierKD2 (electric_bike, Total.Time..int., qqplt = TRUE)
sd(electric_bike_clean$Total.Time..int.)

```
After the data was cleaned, we can see our data is approximately normal, then we'll find the probability of the time boundary 
1). 30 mins (Time boundary for casuals)：
```{r, results=TRUE}

library(ggplot2)

print('The proportion of using classic bike more than 30mins is about：')
pnorm(30, mean(classic_bike_clean$Total.Time..int.), sd(classic_bike_clean$Total.Time..int.),lower.tail=FALSE)
print('')
print('')
print('The proportion of using electric bike more than 30mins is about：')
pnorm(30, mean(electric_bike_clean$Total.Time..int.), sd(electric_bike_clean$Total.Time..int.),lower.tail=FALSE)

```
2). 45 mins (Time boundary for members)：
```{r, results=TRUE}

library(ggplot2)

print('The proportion of using classic bike more than 45mins is about：')
pnorm(45, mean(classic_bike_clean$Total.Time..int.), sd(classic_bike_clean$Total.Time..int.),lower.tail=FALSE)
print('')
print('')
print('The proportion of using electric bike more than 45mins is about：')
pnorm(45, mean(electric_bike_clean$Total.Time..int.), sd(electric_bike_clean$Total.Time..int.),lower.tail=FALSE)

```
According to the probability for both boundaries, we can see that no matter what time boundary is, more than 30 or 45 minutes, the proportion of using classic bike is always more than the proportion of using electric bike. However, we need to test its reliability so that we can have more confidence to recommend the company to adjust their bike storing plan next year (so now suppose August data is just one sample, and the true population is the data from every year). 
We will construct the Confidence Interval and do the T-Test for comparing these two averages
```{r, results=TRUE}

ttest3 <- t.test(classic_bike_clean$Total.Time..int.)
ttest3$conf.int

ttest4 <- t.test(electric_bike_clean$Total.Time..int.)
ttest4$conf.int

```
According to the results above, we can see that there's no overlap between classic-bike's CI and electric-bike's CI. 
```{r, results=TRUE}

ttest2sample_total_time_2 <- t.test(classic_bike_clean$Total.Time..int., electric_bike_clean$Total.Time..int., mu=0, alternative = 'greater' )
ttest2sample_total_time_2

```
According to the p-value for differences in total using time between two types of bikes is 2.182e-10, because our p-value is extremely small, so the data presented is enough evidence to reject null hypothesis H0, and we can’t say two types of bikes always have the same riding time or the classic_bike time is less than electric bike.
(According to this we can recommend next year to add more classic bike, but not quite sure about the electric bike, but this is just the result of EDA and hypothesis testing.)







(But the time of usage may differ between different members, and different bike, we'll subset it into different groups and find which groups has the highest usage of time (so we can certify our target customers or target production of bike). 
However, we can see the longest time of usage is about 1499 minutes, which is so far away from the mean and median, and we'll try to analyze that out when we subset it into groups and analyze it in details.) 

All the questions for numeric variable：
1). What's the total mean of using time for both users (based on users)? (To see if we should adjust the price-plan for different customers) 

2). If we subset to two types of users (with their histogram and boxplots) what's the mean and variance (variation) for two types of users?

3). What's distribution of time_using (what is outliers？If have, then exclude the outliers)

4). Some testings about these two types of users (Use this month (August)'s avearge estimated , even though the month is fixed, but the customers are randomed, for the previous/all months' average for these two groups & Does two members have the same average time?)

5). Then do it again for two types of bike (subset two bikes) (To analyze which bike we should add more)

5). Price correlation with all other members. 

